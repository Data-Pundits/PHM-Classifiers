Random Forest Classifier

    Individual Sensor

        trees = 80,
        pdmp-98% , 85 sec,
        pin-99%, 95 sec,
        po-98% 112 sec

        parameter_grid = {'n_estimators': [200, 400, 500, 600, 700], 'min_samples_leaf': [55, 85, 100],
                           'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False]}


    Combining 3 sensors

        number_models = 4
        random_forest_model = RandomForestClassifier()

        classifier = RandomizedSearchCV(
                    estimator=random_forest_model,
                    param_distributions=parameter_grid,
                    n_iter=number_models,
                    scoring='accuracy',
                    n_jobs=2,
                    cv=10,
                    refit=True,
                    return_train_score=True
                    )

        CASE 1: ALl sensors
            Accuracy Score 0.9939229755454678
            Best params {'n_estimators': 500, 'min_samples_leaf': 85, 'max_features': 'sqrt', 'bootstrap': False}
            Best score 0.987516473861473


        CASE 2: ALl sensors
            Training Accuracy 0.9878247772059746
            Best params {'n_estimators': 400, 'min_samples_leaf': 85, 'max_features': 'auto', 'bootstrap': True}
            Best score 0.9823439008690057


        CASE 3: ALl sensors
            classifier = RandomForestClassifier(n_estimators=700, criterion='entropy', min_samples_leaf= 150,
                                                    max_features= 'sqrt', bootstrap= True)
            Time taken - 818 seconds
            Training Accuracy : 97.85717772245815 %
            R2 value : 0.9552720091021513
            Testing Accuracy : 96.99077463757504 %

        CASE 4: For single sensor
            Time taken - 85 seconds
            number_of_trees, min_leaf = 100, 30
            Training Accuracy : 97.31391991966863 %
            R2 value : 0.8855629046257311
            Testing Accuracy : 95.13861772745021 %


Logistic Regression Classifier
    All sensor data - faster in training (7 to 8 seconds)
        Training with 70-30 split and random state=42
            LogisticRegression(multi_class='auto', solver='lbfgs', random_state=42)
            Training Accuracy - 98.7%
            R2 value = 0.97
            Testing accuracy - 98.4%

        Training with 70-30 split and random state=21
            LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=21)
            Training Accuracy - 98.7%
            R2 value = 0.97
            Testing accuracy - 98.4%

        Training with 80-20 split and random state=42
            LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=21)
            Training Accuracy - 98.48%
            R2 value = 0.95
            Testing accuracy - 98.4%

        Training with 70-30 split and random state=21
            time taken - 207 seconds
            LogisticRegression(multi_class='ovr', solver='liblinear')
            Training Accuracy - 98.9%
            R2 value = 0.99
            Testing accuracy - 99.95%

        Training with 70-30 split and random state=21
            time taken - 230 seconds
            LogisticRegressionCV(cv=5, random_state=0)
            Training Accuracy - 100%
            R2 value = 0.99
            Testing accuracy - 99.9%

        Training with 70-30 split
            time taken - 350 seconds
            LogisticRegressionCV(cv=10, random_state=42)
            Training Accuracy - 100%
            R2 value = 0.99
            Testing accuracy - 99.9%

        Training with 70-30 split and random state=42
            time taken -  seconds
            LogisticRegressionCV(class_weight='balanced', multi_class='multinomial', solver='lbfgs')
            Training Accuracy - %
            R2 value = 0.
            Testing accuracy - %

Statistical input features

Reducing number of features